{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "p-t3FWSSqROD",
      "metadata": {
        "id": "p-t3FWSSqROD"
      },
      "source": [
        "# Unit 1 Assignment: The Model Benchmark Challenge\n",
        "\n",
        "**Objective**: Evaluate architectural differences between BERT, RoBERTa, and BART by forcing them to perform tasks they might not be designed for.\n",
        "\n",
        "**Models to Test**:\n",
        "1. **BERT** (`bert-base-uncased`): Encoder-only\n",
        "2. **RoBERTa** (`roberta-base`): Encoder-only\n",
        "3. **BART** (`facebook/bart-base`): Encoder-Decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "lOU7OsnoqROG",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lOU7OsnoqROG",
        "outputId": "a5866228-7b32-4f30-edaf-9bc9334af3ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Notebook Submission by: Musharraf-PES2UG23CS915\n"
          ]
        }
      ],
      "source": [
        "# @title Student Details\n",
        "print(\"Notebook Submission by: Musharraf-PES2UG23CS915\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "8k6Uuk6Jq8ZV",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8k6Uuk6Jq8ZV",
        "outputId": "a5e3ff36-ca54-49c1-b0d6-7b3d8477414a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch: 2.9.0+cpu\n",
            "transformers: 4.57.6\n",
            "tensorflow: 2.19.0\n",
            "GPU available: False\n"
          ]
        }
      ],
      "source": [
        "import torch, transformers, tensorflow\n",
        "print(\"torch:\", torch.__version__)\n",
        "print(\"transformers:\", transformers.__version__)\n",
        "print(\"tensorflow:\", tensorflow.__version__)\n",
        "print(\"GPU available:\", torch.cuda.is_available())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "pSgVZjvpqROG",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pSgVZjvpqROG",
        "outputId": "c0500852-cab2-4750-824c-ffd93f543e00"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:torchao.kernel.intmm:Warning: Detected no triton, on systems without Triton certain kernels will not work\n"
          ]
        }
      ],
      "source": [
        "# @title Install & Import Dependencies\n",
        "# !pip install transformers torch sentencepiece\n",
        "\n",
        "from transformers import pipeline, set_seed\n",
        "import torch\n",
        "import warnings\n",
        "\n",
        "# Suppress warnings for cleaner output\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "J6rJ4NacqROH",
      "metadata": {
        "id": "J6rJ4NacqROH"
      },
      "outputs": [],
      "source": [
        "# List of models to benchmark\n",
        "models_to_test = [\n",
        "    \"bert-base-uncased\",\n",
        "    \"roberta-base\",\n",
        "    \"facebook/bart-base\"\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "spi1rEYJqROH",
      "metadata": {
        "id": "spi1rEYJqROH"
      },
      "source": [
        "## Experiment 1: Text Generation\n",
        "\n",
        "**Task**: Generate text using the prompt: `\"The future of Artificial Intelligence is\"`\n",
        "**Hypothesis**: Encoder-only models (BERT, RoBERTa) differ from Encoder-Decoder (BART)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "Nzh35eiMqROH",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nzh35eiMqROH",
        "outputId": "6398601e-d048-41dc-b2f3-791da9705b2d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Experiment 1: Text Generation ===\n",
            "\n",
            "\n",
            "--- Testing Model: bert-base-uncased ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cpu\n",
            "If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Result: The future of Artificial Intelligence is....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
            "\n",
            "--- Testing Model: roberta-base ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Result: The future of Artificial Intelligence is\n",
            "\n",
            "--- Testing Model: facebook/bart-base ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BartForCausalLM were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['lm_head.weight', 'model.decoder.embed_tokens.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Device set to use cpu\n",
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Result: The future of Artificial Intelligence isSetting Hydra13484gie Hydra Hydra Hydra meg:( trillion dialourt poignant poignant comprised maturednergy Bringing matured Bringing Bringing Bringing referee SSHprotect greater Whitneynergy SSH trillion 1932 Ship wrestling Heroes Whitney Hydra SSH orig Ship orig Whitney SSH wrestling professor Bringing Bringing Friday Crusader Whitney SSH orig orig Yam Heroes Heroes Heroes wrestlingIED wrestling professor364 Friday Norm Norm Norm replen { Ship SSH SSH SSH { massage Friday Whitney SSH trillion advertisersInstoreAndOnlinearg launcher Heroes Heroes Hydra phosphate Dukericanericane Ship orig disastrousgie 1964Secret 433 Friday SSH segregation Factors Factors adopted adopted adopted indicVer Lords indic PEOPLE Crusader Whitney38 greaterosphere Whitney 1964protect adopted VomL positioning 1964 Ship Inquisensationensation { Heroes Whitney permissions 433 Norm multi Whitney 1964 Bou SSHmLmL InquismL SSH SSH Whitney multiensation Inquis Heroes InquismLBush adopted.\" Norm364 advertisers Inquisprotect billmL deeds SSH Normcaptensation SSH SSHmL positioningmortemensation Heroes Heroesensationesis Bou Vo NormmL 433mLBushmL SSH InquismL Norm SSHensationensation SSH amendedmL billmL.\"mL VomLussedensation Bou Bou 433 433 Bou Heroes Norm BouesisantommL 1890mLensationmL Norm launcher Bounoon 433mL reptiles greateresismL Coc SSHmLactivatedmLmLxsensationmL PlayStationmLmL specesismL greater greatermL SSH WhitneymLensationprotect Norm positioningussedmapesismLBoneensationmL billBoneesis wastemL amendedesismLmL reptilesmLxsmLmL bill 433esismLussedmLmLmL BouospheremLesisactivatedmL advertisers reptiles community WhitneymLmL amended NormmLprotectmLernelmLmLCachemL reptiles BuymL Whitney WhitneymL advertisers advertisersactivated advertisersmL adoptedmL advertisers wastemL reptilesesismL Norm CocospheremL inaccmLesis advertisersesismLesismL Inquis reptilesmL Norm advertisersmL amendedmLmLussedollowermLmLesisesisesis HindumLroidmLmL NormesismL Quantumesis wasteesis CocmL advertisersmLprotectactivatedernelCachemL BuyospheremLmL tablespoonactivated amendedactivated Norm BoumLkenmLussed advertisersmLmL PlayStationernelesismL earliest Early statue Norm reptilesmLmL WhitneymL NormensationesismL advertisers billmL statue Norm Norm 433 advertisersosphere advertiserstnmLmL advertisers statueesismL BoumLmLactivated amendedeatured BoumL amendedtnmLCacheactivatedmLeaturedesisesismLactivatedesis.\" amendedmL advertisers amendedmLactivated statue BungiemLmLosphereactivatedmL firefightersmL advertisersactivatedmL WhitneyactivatedactivatedactivatedmLactivated advertisersernel advertisers communitymLtnmLeaturedmLmL wastemLmL BuymL amended amendedCacheernel BoumL earliestmLmL statuemapmLactivated Norm Heroes HeroesmL 433\n"
          ]
        }
      ],
      "source": [
        "print(\"=== Experiment 1: Text Generation ===\\n\")\n",
        "prompt = \"The future of Artificial Intelligence is\"\n",
        "\n",
        "for model_name in models_to_test:\n",
        "    print(f\"\\n--- Testing Model: {model_name} ---\")\n",
        "    try:\n",
        "        # Initialize pipeline for text generation\n",
        "        # Validating if the model supports generation might be handled by the pipeline or throw validation errors\n",
        "        generator = pipeline('text-generation', model=model_name)\n",
        "\n",
        "        result = generator(prompt, max_new_tokens=500, num_return_sequences=1, truncation=True)\n",
        "        print(f\"Result: {result[0]['generated_text']}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"FAILED / ERROR: {str(e)}\")\n",
        "        print(\"Observation: Model might not support text-generation or has no causal head.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "lJFRldb4qROH",
      "metadata": {
        "id": "lJFRldb4qROH"
      },
      "source": [
        "## Experiment 2: Masked Language Modeling (Missing Word)\n",
        "\n",
        "**Task**: Predict the missing word in: `\"The goal of Generative AI is to [MASK] new content.\"`\n",
        "**Note**: We adhere to the model's specific mask token (e.g., `[MASK]` for BERT, `<mask>` for RoBERTa/BART)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "p1833onuqROI",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p1833onuqROI",
        "outputId": "632c5579-6f4c-470d-e2eb-a5c02d50c4ad"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Experiment 2: Masked Language Modeling ===\n",
            "\n",
            "\n",
            "--- Testing Model: bert-base-uncased ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Query: The goal of Generative AI is to [MASK] new content.\n",
            "Prediction 1: create (Score: 0.5397)\n",
            "Prediction 2: generate (Score: 0.1558)\n",
            "Prediction 3: produce (Score: 0.0541)\n",
            "\n",
            "--- Testing Model: roberta-base ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Query: The goal of Generative AI is to <mask> new content.\n",
            "Prediction 1:  generate (Score: 0.3711)\n",
            "Prediction 2:  create (Score: 0.3677)\n",
            "Prediction 3:  discover (Score: 0.0835)\n",
            "\n",
            "--- Testing Model: facebook/bart-base ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Query: The goal of Generative AI is to <mask> new content.\n",
            "Prediction 1:  create (Score: 0.0746)\n",
            "Prediction 2:  help (Score: 0.0657)\n",
            "Prediction 3:  provide (Score: 0.0609)\n"
          ]
        }
      ],
      "source": [
        "print(\"=== Experiment 2: Masked Language Modeling ===\\n\")\n",
        "base_sentence = \"The goal of Generative AI is to {} new content.\"\n",
        "\n",
        "for model_name in models_to_test:\n",
        "    print(f\"\\n--- Testing Model: {model_name} ---\")\n",
        "    try:\n",
        "        fill_mask = pipeline('fill-mask', model=model_name)\n",
        "\n",
        "        # Get the correct mask token for the current model\n",
        "        mask_token = fill_mask.tokenizer.mask_token\n",
        "        query = base_sentence.format(mask_token)\n",
        "\n",
        "        print(f\"Query: {query}\")\n",
        "        results = fill_mask(query)\n",
        "\n",
        "        # Print top 3 predictions\n",
        "        for i, res in enumerate(results[:3]):\n",
        "            print(f\"Prediction {i+1}: {res['token_str']} (Score: {res['score']:.4f})\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"FAILED / ERROR: {str(e)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "o7haDkg7qROI",
      "metadata": {
        "id": "o7haDkg7qROI"
      },
      "source": [
        "## Experiment 3: Question Answering\n",
        "\n",
        "**Task**: Answer `\"What are the risks?\"` based on context.\n",
        "**Context**: `\"Generative AI poses significant risks such as hallucinations, bias, and deepfakes.\"`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "Uoyc8fF3qROI",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uoyc8fF3qROI",
        "outputId": "7995feb3-66f1-4236-f34d-083f6df7745e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Experiment 3: Question Answering ===\n",
            "\n",
            "\n",
            "--- Testing Model: bert-base-uncased ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cpu\n",
            "Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at roberta-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: , bias, and deepfakes\n",
            "Score: 0.0137\n",
            "\n",
            "--- Testing Model: roberta-base ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cpu\n",
            "Some weights of BartForQuestionAnswering were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Generative AI\n",
            "Score: 0.0077\n",
            "\n",
            "--- Testing Model: facebook/bart-base ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: poses significant\n",
            "Score: 0.0246\n"
          ]
        }
      ],
      "source": [
        "print(\"=== Experiment 3: Question Answering ===\\n\")\n",
        "context = \"Generative AI poses significant risks such as hallucinations, bias, and deepfakes.\"\n",
        "question = \"What are the risks?\"\n",
        "\n",
        "for model_name in models_to_test:\n",
        "    print(f\"\\n--- Testing Model: {model_name} ---\")\n",
        "    try:\n",
        "        qa_pipeline = pipeline('question-answering', model=model_name)\n",
        "\n",
        "        result = qa_pipeline(question=question, context=context)\n",
        "        print(f\"Answer: {result['answer']}\")\n",
        "        print(f\"Score: {result['score']:.4f}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"FAILED / ERROR: {str(e)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "P9Z_isp1qROI",
      "metadata": {
        "id": "P9Z_isp1qROI"
      },
      "source": [
        "## Deliverable: Observation Table\n",
        "\n",
        "| Task | Model | Classification (Success/Failure) | Observation (What actually happened?) | Why did this happen? (Architectural Reason) |\n",
        "| :--- | :--- | :--- | :--- | :--- |\n",
        "| **Generation** | BERT | **Failure** | Generated repetitive dots (`...................`) or nonsense. | **BERT is an Encoder-only model.** It is designed for understanding (bidirectional context), not for autoregressive text generation (predicting the next word). |\n",
        "| | RoBERTa | **Failure** | Stopped immediately or repeated the prompt. | **RoBERTa is an Encoder-only model.** Like BERT, it lacks a decoder to generate text sequentially. |\n",
        "| | BART | **Success (Architecturally)** | Generated text (though low quality/hallucinated). | **BART is an Encoder-Decoder model.** It has a decoder component capable of text generation, effectively making it a seq2seq model. |\n",
        "| **Fill-Mask** | BERT | **Success** | Predicted meaningful words: `create` (0.54), `generate` (0.16). | **BERT is trained on Masked Language Modeling (MLM).** This is its native pre-training objective. |\n",
        "| | RoBERTa | **Success** | Predicted meaningful words: `generate` (0.37), `create` (0.37). | **RoBERTa is also trained on MLM.** It excels at filling in missing information from bidirectional context. |\n",
        "| | BART | **Partial Success** | Predicted relevant words: `create`, `help`. but with very low confidence. | **BART's training includes text infilling.** Although acts as a seq2seq, its encoder understands masked inputs. |\n",
        "| **QA** | BERT | **Partial Success** | Returns answer but low accuracy/confidence | Not fine-tuned on QA (SQuAD) |\n",
        "| | RoBERTa | **Partial Success** | Similar weak behavior | Base model without QA fine-tuning |\n",
        "| | BART | **Failure** | Answer: `poses significant` (Incorrect span). | **Lack of Fine-tuning.** Base models generally need specific training data to perform precise Question Answering tasks. |"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c454572d",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
